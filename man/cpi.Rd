% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cpi.R
\name{cpi}
\alias{cpi}
\title{Conditional Predictive Impact (CPI).}
\usage{
cpi(
  task,
  learner,
  resampling = NULL,
  test_data = NULL,
  measure = NULL,
  test = "t",
  log = FALSE,
  B = 1999,
  alpha = 0.05,
  x_tilde = NULL,
  knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)),
  groups = NULL,
  verbose = FALSE,
  modify_trp = FALSE,
  ...
)
}
\arguments{
\item{task}{The prediction \code{mlr3} task, see examples.}

\item{learner}{The \code{mlr3} learner used in CPI. If you pass a string, the 
learner will be created via \code{mlr3::\link{lrn}}.}

\item{resampling}{Resampling strategy, \code{mlr3} resampling object 
(e.g. \code{rsmp("holdout")}), "oob" (out-of-bag) or "none" 
(in-sample loss).}

\item{test_data}{External validation data, use instead of resampling.}

\item{measure}{Performance measure (loss). Per default, use MSE 
(\code{"regr.mse"}) for regression and logloss (\code{"classif.logloss"}) 
for classification.}

\item{test}{Statistical test to perform, one of \code{"t"} (t-test, default), 
\code{"wilcox"} (Wilcoxon signed-rank test), \code{"binom"} (binomial 
test), \code{"fisher"} (Fisher permutation test) or "bayes" 
(Bayesian testing, computationally intensive!). See Details.}

\item{log}{Set to \code{TRUE} for multiplicative CPI (\eqn{\lambda}), to 
\code{FALSE} (default) for additive CPI (\eqn{\Delta}).}

\item{B}{Number of permutations for Fisher permutation test.}

\item{alpha}{Significance level for confidence intervals.}

\item{x_tilde}{Knockoff matrix or data.frame. If not given (the default), it will be 
created with the function given in \code{knockoff_fun}.}

\item{knockoff_fun}{Function to generate knockoffs. Default: 
\code{knockoff::\link{create.second_order}} with matrix argument.}

\item{groups}{(Named) list with groups. Set to \code{NULL} (default) for no
groups, i.e. compute CPI for each feature. See examples.}

\item{verbose}{Verbose output of resampling procedure.}

\item{modify_trp}{An optional function to modify the \code{truth}, \code{response}, 
and \code{prob} values of the model output. Function must take as arguments the 
original \code{truth}, \code{response}, and \code{prob} output from model 
predictions and return a named list of \code{truth}, \code{response}, and 
\code{prob} values, some of which may be modified. For modified values to work 
nicely with the rest of the package, they must match the expected format of 
unmodified values (e.g., \code{prob} must be a data.frame with the number of columns
being the number of classes and column names being the class labels; the first 
level of the \code{truth} factor must be the positive class). See examples. Set to 
\code{FALSE} (default) to use unmodified truth, response, and probability values 
from model output.}

\item{...}{Optional arguments to be passed to the \code{modify_trp} function 
(e.g., a classification threshold).}
}
\value{
For \code{test = "bayes"} a list of \code{BEST} objects. In any other 
case, a \code{data.frame} with a row for each feature and columns:
  \item{Variable/Group}{Variable/group name}
  \item{CPI}{CPI value}
  \item{SE}{Standard error}
  \item{test}{Testing method}
  \item{statistic}{Test statistic (only for t-test, Wilcoxon and binomial test)}
  \item{estimate}{Estimated mean (for t-test), median (for Wilcoxon test),
    or proportion of \eqn{\Delta}-values greater than 0 (for binomial test).}
  \item{p.value}{p-value}
  \item{ci.lo}{Lower limit of (1 - \code{alpha}) * 100\% confidence interval}
Note that NA values are no error but a result of a CPI value of 0, i.e. no 
difference in model performance after replacing a feature with its knockoff.
}
\description{
A general test for conditional 
independence in supervised learning algorithms. Implements a conditional 
variable importance measure which can be applied to any supervised learning 
algorithm and loss function. Provides statistical inference procedures 
without parametric assumptions and applies equally well to continuous and 
categorical predictors and outcomes.
}
\details{
This function computes the conditional predictive impact (CPI) of one or
several features on a given supervised learning task. This represents the 
mean error inflation when replacing a true variable with its knockoff. Large
CPI values are evidence that the feature(s) in question have high 
\emph{conditional variable importance} -- i.e., the fitted model relies on 
the feature(s) to predict the outcome, even after accounting for the signal
from all remaining covariates. 

We build on the \code{mlr3} framework, which provides a unified interface for 
training models, specifying loss functions, and estimating generalization 
error. See the package documentation for more info.

Methods are implemented for frequentist and Bayesian inference. The default
is \code{test = "t"}, which is fast and powerful for most sample sizes. The
Wilcoxon signed-rank test (\code{test = "wilcox"}) may be more appropriate if 
the CPI distribution is skewed, while the binomial test (\code{test = "binom"}) 
requires basically no assumptions but may have less power. For small sample 
sizes, we recommend permutation tests (\code{test = "fisher"}) or Bayesian 
methods (\code{test = "bayes"}). In the latter case, default priors are 
assumed. See the \code{BEST} package for more info.

For parallel execution, register a backend, e.g. with
\code{doParallel::registerDoParallel()}.
}
\examples{
library(mlr3)
library(mlr3learners)

# Regression with linear model and holdout validation
cpi(task = tsk("mtcars"), learner = lrn("regr.lm"), 
    resampling = rsmp("holdout"))

\donttest{
# Classification with logistic regression, log-loss and t-test
cpi(task = tsk("wine"), 
    learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
    resampling = rsmp("holdout"), 
    measure = "classif.logloss", test = "t")
 
# Use your own data (and out-of-bag loss with random forest)
mytask <- as_task_classif(iris, target = "Species")
mylearner <- lrn("classif.ranger", predict_type = "prob", keep.inbag = TRUE)
cpi(task = mytask, learner = mylearner, 
    resampling = "oob", measure = "classif.logloss")
    
# Group CPI
cpi(task = tsk("iris"), 
    learner = lrn("classif.ranger", predict_type = "prob", num.trees = 10), 
    resampling = rsmp("cv", folds = 3), 
    groups = list(Sepal = 1:2, Petal = 3:4))
}     
\dontrun{      
# Bayesian testing
res <- cpi(task = tsk("iris"), 
           learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
           resampling = rsmp("holdout"), 
           measure = "classif.logloss", test = "bayes")
plot(res$Petal.Length)

# Parallel execution
doParallel::registerDoParallel()
cpi(task = tsk("wine"), 
    learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
    resampling = rsmp("cv", folds = 5))
    
# Use sequential knockoffs for categorical features
# package available here: https://github.com/kormama1/seqknockoff
mytask <- as_task_regr(iris, target = "Petal.Length")
cpi(task = mytask, learner = lrn("regr.ranger"), 
    resampling = rsmp("holdout"), 
    knockoff_fun = seqknockoff::knockoffs_seq)

# Use a function to modify the truth, response, and probability model outputs
# Add a probability data.frame to predictions when regression was run on a
# 0/1 binary outcome (i.e., treat the response as a probability, make the
# response a "classified" outcome)

# Data prep:
data <- palmerpenguins::penguins
data$species <- ifelse(data$species == "Adelie", 1, 0)
keep_cols <- c("species", "bill_length_mm", "bill_depth_mm", 
               "flipper_length_mm", "body_mass_g")
data <- data[, keep_cols]
data <- data[complete.cases(data), ]

response_is_prob <- function(truth, response, prob) {
   prob <- setNames(data.frame(response, 1 - response), nm = c("1", "0"))
   response <- ifelse(response >= 0.5, yes = 1, no = 0)
   
   return(list(truth = truth, response = response, prob = prob))
  }     

cpi(task = TaskRegr$new(id = "penguins.regr", backend = data, target = "species"), 
    learner = lrn("regr.ranger"), 
     resampling = rsmp("holdout"), 
     measure = "classif.logloss", test = "t", 
     modify_trp = response_is_prob)
     
# Same as above, but also passing an additional argument to the
# modify_trp function that makes a new classification threshold
# Same data can be used.
 
response_is_prob_new_thresh <- 
  function(truth, response, prob, classification_thresh) {
    prob <- setNames(data.frame(response, 1 - response), nm = c("1", "0"))
    response <- ifelse(response >= classification_thresh, yes = 1, no = 0)
    
    return(list(truth = truth, response = response, prob = prob))
  }
   
cpi(task = TaskRegr$new(id = "penguins.regr", backend = data, target = "species"), 
    learner = lrn("regr.ranger"), 
    resampling = rsmp("holdout"), 
    measure = "classif.logloss", test = "t", 
    modify_trp = response_is_prob_new_thresh,
    classification_thresh = 0.3)
       
# Setting a new classification threshold on a "classif" learner and rescaling
# probability outputs such that predictions lower than the new threshold
# are on [0,0.5] and predictions above the new threshold are on [0.5, 1]
 
rescale_prob <- function(truth, response, prob, classification_thresh) {
 
    rescale <- function(x, old_max, old_min, new_max, new_min) {
      return(((new_max - new_min) / (old_max - old_min)) * (x - old_max) + new_max)
    }

   classes <- levels(truth)
   response <- ifelse(prob[, classes[1]] >= classification_thresh, 
                      yes = classes[1], no = classes[2])
   prob[, classes[1]] <- ifelse(prob[, classes[1]] <= classification_thresh,
                                yes = rescale(x = prob[, classes[1]],
                                              old_max = classification_thresh,
                                              old_min = 0,
                                              new_max = 0.5,
                                              new_min = 0),
                                no = rescale(x = prob[, classes[1]],
                                             old_max = 1,
                                             old_min = classification_thresh,
                                             new_max = 1,
                                             new_min = 0.5))

   prob[, classes[2]] <- 1 - prob[, classes[1]]

   return(list(truth = truth, response = response, prob = prob))
 }
 
 # Data prep
 data = palmerpenguins::penguins
 data$species = factor(ifelse(data$species == "Adelie", "1", "0"))
 keep_cols <- c("species", "bill_length_mm", "bill_depth_mm", 
                "flipper_length_mm", "body_mass_g")
 data <- data[, keep_cols]
 data <- data[complete.cases(data), ]
 cpi(task = TaskClassif$new(id = "penguins.binary", backend = data, 
                            target = "species", positive = "1"), 
     learner = lrn("classif.ranger", predict_type = "prob"), 
     resampling = rsmp("holdout"), 
     measure = "classif.logloss", test = "t", 
     modify_trp = rescale_prob,
     classification_thresh = 0.3)
}
}
\references{
Watson, D. & Wright, M. (2020). Testing conditional independence in 
supervised learning algorithms. \emph{Machine Learning}, \emph{110}(8): 
2107-2129. \doi{10.1007/s10994-021-06030-6}

Candès, E., Fan, Y., Janson, L, & Lv, J. (2018). {Panning for gold: 'model-X'
knockoffs for high dimensional controlled variable selection}. \emph{J. R. 
Statistc. Soc. B}, \emph{80}(3): 551-577. \doi{10.1111/rssb.12265}
}
