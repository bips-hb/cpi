[{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"get-started","dir":"Articles","previous_headings":"","what":"Get started","title":"Introduction to the cpi package","text":"Conditional Predictive Impact (CPI) general test conditional independence supervised learning algorithms. implements conditional variable importance measure can applied supervised learning algorithm loss function. first example, calculate CPI random forest iris data 5-fold cross validation: result CPI value feature, .e. much loss function change feature replaced knockoff version, corresponding standard errors, test statistics, p-values confidence interval.","code":"library(mlr3) library(mlr3learners) library(cpi)  cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\"),     resampling = rsmp(\"cv\", folds = 5)) #>       Variable           CPI           SE test  statistic   p.value #> 1 Petal.Length  1.725044e-03 0.0023569156    t  0.7319075 0.2326873 #> 2  Petal.Width -5.091617e-03 0.0109664048    t -0.4642923 0.6784420 #> 3 Sepal.Length -8.716911e-05 0.0005415392    t -0.1609655 0.5638307 #> 4  Sepal.Width -2.020510e-03 0.0027946834    t -0.7229836 0.7645883 #>        estimate         ci.lo #> 1  1.725044e-03 -0.0021759918 #> 2 -5.091617e-03 -0.0232426023 #> 3 -8.716911e-05 -0.0009834947 #> 4 -2.020510e-03 -0.0066461152"},{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"interface-with-mlr3","dir":"Articles","previous_headings":"","what":"Interface with mlr3","title":"Introduction to the cpi package","text":"task, learner resampling strategy specified mlr3 package, provides unified interface machine learning tasks makes quite easy change components. example, can change regularized logistic regression simple holdout resampling strategy: refer mlr3 book full introduction reference. loss function used cpi() function specified measure. default, mean squared error (MSE) used regression log-loss classification. mlr3, corresponds measures \"regr.mse\" \"classif.logloss\". re-run example simple classification error (ce): Note need predict_type = \"prob\" classification error uses hard classification.","code":"cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.01),     resampling = rsmp(\"holdout\")) #>       Variable           CPI           SE test statistic   p.value #> 1 Petal.Length -8.487401e-06 7.118436e-06    t -1.192313 0.8805595 #> 2  Petal.Width  1.046826e-02 1.851230e-02    t  0.565476 0.2871652 #> 3 Sepal.Length  0.000000e+00 0.000000e+00    t       NaN       NaN #> 4  Sepal.Width  9.281975e-03 7.871645e-03    t  1.179166 0.1220130 #>        estimate         ci.lo #> 1 -8.487401e-06 -2.042182e-05 #> 2  1.046826e-02 -2.056855e-02 #> 3  0.000000e+00           NaN #> 4  9.281975e-03 -3.915238e-03 cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", lambda = 0.01),     resampling = rsmp(\"holdout\"),      measure = msr(\"classif.ce\")) #>       Variable CPI         SE test statistic p.value estimate       ci.lo #> 1 Petal.Length   0 0.00000000    t       NaN     NaN        0         NaN #> 2  Petal.Width   0 0.00000000    t       NaN     NaN        0         NaN #> 3 Sepal.Length   0 0.00000000    t       NaN     NaN        0         NaN #> 4  Sepal.Width   0 0.02857143    t         0     0.5        0 -0.04790145"},{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"statistical-testing","dir":"Articles","previous_headings":"","what":"Statistical testing","title":"Introduction to the cpi package","text":"CPI offers several statistical tests calculated: t-test (\"t\", default), Wilcoxon signed-rank test (\"wilcox\"), binomial test (\"binom\"), Fisher permutation test (\"fisher\") Bayesian testing (\"bayes\") package BEST. example, re-run first example Fisher’s permutation test:","code":"cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\"),     resampling = rsmp(\"cv\", folds = 5),      test = \"fisher\") #>       Variable           CPI           SE   test p.value        ci.lo #> 1 Petal.Length -0.0011453409 0.0018595287 fisher  0.6480 -0.003783658 #> 2  Petal.Width -0.0032325510 0.0131528536 fisher  0.5020 -0.022315210 #> 3 Sepal.Length -0.0008465851 0.0006780689 fisher  0.8575 -0.001968333 #> 4  Sepal.Width -0.0037897660 0.0043431298 fisher  0.7615 -0.010496257"},{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"knockoff-procedures","dir":"Articles","previous_headings":"","what":"Knockoff procedures","title":"Introduction to the cpi package","text":"CPI relies valid knockoff sampler data analyzed. default, second-order Gaussian knockoffs package knockoff used. However, knockoff sampler can used changing knockoff_fun x_tilde argument cpi() function. , knockoff_fun expects function taking data.frame original data input returning data.frame knockoffs. example, use sequential knockoffs seqknockoff package1: x_tilde argument directly takes knockoff data:","code":"mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\"),      resampling = rsmp(\"holdout\"),      knockoff_fun = seqknockoff::knockoffs_seq) library(seqknockoff) x_tilde <- knockoffs_seq(iris[, -3]) mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\"),      resampling = rsmp(\"holdout\"),      x_tilde = x_tilde)"},{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"group-cpi","dir":"Articles","previous_headings":"","what":"Group CPI","title":"Introduction to the cpi package","text":"Instead calculating CPI feature separately, can also calculate groups features replacing data whole groups respective knockoff data. cpi() can done groups argument:","code":"cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.01),     resampling = rsmp(\"holdout\"),      groups = list(Sepal = 1:2, Petal = 3:4)) #>   Group         CPI          SE test statistic   p.value    estimate #> 1 Sepal 0.001820347 0.005550561    t 0.3279574 0.3721706 0.001820347 #> 2 Petal 0.008695943 0.020500330    t 0.4241855 0.3366435 0.008695943 #>         ci.lo #> 1 -0.00748545 #> 2 -0.02567390"},{"path":"https://dswatson.github.io/cpi/articles/intro.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"Parallelization","title":"Introduction to the cpi package","text":"parallel execution, need register parallel backend. Parallelization performed features, .e. CPI feature calculated parallel. example:","code":"doParallel::registerDoParallel(4) cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\"),     resampling = rsmp(\"cv\", folds = 5))"},{"path":"https://dswatson.github.io/cpi/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Maintainer.","code":""},{"path":"https://dswatson.github.io/cpi/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Watson DS, Wright MN (2022). cpi: Conditional Predictive Impact. https://github.com/dswatson/cpi, https://dswatson.github.io/cpi/.","code":"@Manual{,   title = {cpi: Conditional Predictive Impact},   author = {David S. Watson and Marvin N. Wright},   year = {2022},   note = {https://github.com/dswatson/cpi, https://dswatson.github.io/cpi/}, }"},{"path":"https://dswatson.github.io/cpi/index.html","id":"conditional-predictive-impact","dir":"","previous_headings":"","what":"Conditional Predictive Impact","title":"Conditional Predictive Impact","text":"David S. Watson, Marvin N. Wright","code":""},{"path":"https://dswatson.github.io/cpi/index.html","id":"introduction","dir":"","previous_headings":"Conditional Predictive Impact","what":"Introduction","title":"Conditional Predictive Impact","text":"conditional predictive impact (CPI) measure conditional independence. can calculated using supervised learning algorithm, loss function, knockoff sampler. provide statistical inference procedures CPI without parametric assumptions sparsity constraints. method works continuous categorical data.","code":""},{"path":"https://dswatson.github.io/cpi/index.html","id":"installation","dir":"","previous_headings":"Conditional Predictive Impact","what":"Installation","title":"Conditional Predictive Impact","text":"package CRAN yet. install development version GitHub using devtools, run","code":"devtools::install_github(\"dswatson/cpi\")"},{"path":"https://dswatson.github.io/cpi/index.html","id":"examples","dir":"","previous_headings":"Conditional Predictive Impact","what":"Examples","title":"Conditional Predictive Impact","text":"Calculate CPI random forest iris data 5-fold cross validation:","code":"library(mlr3) library(mlr3learners) library(cpi)  cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\"),     resampling = rsmp(\"cv\", folds = 5),      measure = \"classif.logloss\", test = \"t\")"},{"path":"https://dswatson.github.io/cpi/index.html","id":"references","dir":"","previous_headings":"Conditional Predictive Impact","what":"References","title":"Conditional Predictive Impact","text":"Watson D. S. & Wright, M. N. (2021). Testing conditional independence supervised learning algorithms. Machine Learning. DOI: 10.1007/s10994-021-06030-6.","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Predictive Impact (CPI). — cpi","title":"Conditional Predictive Impact (CPI). — cpi","text":"general test conditional  independence supervised learning algorithms. Implements conditional  variable importance measure can applied supervised learning  algorithm loss function. Provides statistical inference procedures  without parametric assumptions applies equally well continuous  categorical predictors outcomes.","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Predictive Impact (CPI). — cpi","text":"","code":"cpi(   task,   learner,   resampling = NULL,   test_data = NULL,   measure = NULL,   test = \"t\",   log = FALSE,   B = 1999,   alpha = 0.05,   x_tilde = NULL,   knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)),   groups = NULL,   verbose = FALSE )"},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Predictive Impact (CPI). — cpi","text":"task prediction mlr3 task, see examples. learner mlr3 learner used CPI. pass string,  learner created via mlr3::lrn. resampling Resampling strategy, mlr3 resampling object  (e.g. rsmp(\"holdout\")), \"oob\" (--bag) \"none\"  (-sample loss). test_data External validation data, use instead resampling. measure Performance measure (loss). Per default, use MSE  (\"regr.mse\") regression logloss (\"classif.logloss\")  classification. test Statistical test perform, one \"t\" (t-test, default),  \"wilcox\" (Wilcoxon signed-rank test), \"binom\" (binomial  test), \"fisher\" (Fisher permutation test) \"bayes\"  (Bayesian testing, computationally intensive!). See Details. log Set TRUE multiplicative CPI (\\(\\lambda\\)),  FALSE (default) additive CPI (\\(\\Delta\\)). B Number permutations Fisher permutation test. alpha Significance level confidence intervals. x_tilde Knockoff matrix data.frame. given (default),  created function given knockoff_fun. knockoff_fun Function generate knockoffs. Default:  knockoff::create.second_order matrix argument. groups (Named) list groups. Set NULL (default) groups, .e. compute CPI feature. See examples. verbose Verbose output resampling procedure.","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Predictive Impact (CPI). — cpi","text":"test = \"bayes\" list BEST objects.  case, data.frame row feature columns: Variable/Group Variable/group name CPI CPI value SE Standard error test Testing method statistic Test statistic (t-test) p.value p-value estimate Estimated mean (t-test), median (Wilcoxon test),     proportion \\(\\Delta\\)-values greater 0 (binomial test). ci.lo Lower limit (1 - alpha) * 100% confidence interval","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Predictive Impact (CPI). — cpi","text":"function computes conditional predictive impact (CPI) one several features given supervised learning task. represents  mean error inflation replacing true variable knockoff. Large CPI values evidence feature(s) question high  conditional variable importance -- .e., fitted model relies  feature(s) predict outcome, even accounting signal remaining covariates. build mlr3 framework, provides unified interface  training models, specifying loss functions, estimating generalization  error. See package documentation info. Methods implemented frequentist Bayesian inference. default test = \"t\", fast powerful sample sizes. Wilcoxon signed-rank test (test = \"wilcox\") may appropriate  CPI distribution skewed, binomial test (test = \"binom\")  requires basically assumptions may less power. small sample  sizes, recommend permutation tests (test = \"fisher\") Bayesian  methods (test = \"bayes\"). latter case, default priors  assumed. See BEST package info. parallel execution, register backend, e.g. doParallel::registerDoParallel().","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional Predictive Impact (CPI). — cpi","text":"Watson, D. & Wright, M. (2020). Testing conditional independence  supervised learning algorithms. Machine Learning, 110(8):  2107-2129. doi: 10.1007/s10994-021-06030-6 Candès, E., Fan, Y., Janson, L, & Lv, J. (2018). Panning gold: 'model-X' knockoffs high dimensional controlled variable selection. J. R.  Statistc. Soc. B, 80(3): 551-577. doi: 10.1111/rssb.12265","code":""},{"path":"https://dswatson.github.io/cpi/reference/cpi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Predictive Impact (CPI). — cpi","text":"","code":"library(mlr3) library(mlr3learners)  # Regression with linear model cpi(task = tsk(\"mtcars\"), learner = lrn(\"regr.lm\"),      resampling = rsmp(\"holdout\")) #>    Variable           CPI           SE test   statistic   p.value      estimate #> 1        am  3.130204e-01 4.958199e-01    t  0.63131887 0.2709964  3.130204e-01 #> 2      carb -1.334695e-05 4.011574e-05    t -0.33271107 0.6268890 -1.334695e-05 #> 3       cyl -7.674866e+00 2.407064e+00    t -3.18847589 0.9951604 -7.674866e+00 #> 4      disp -2.585680e-03 2.257808e-03    t -1.14521672 0.8606061 -2.585680e-03 #> 5      drat -2.460224e+00 2.886983e+00    t -0.85217850 0.7929709 -2.460224e+00 #> 6      gear -4.659132e+00 2.924751e+00    t -1.59300097 0.9288779 -4.659132e+00 #> 7        hp  1.245472e+00 2.752442e+00    t  0.45249722 0.3302843  1.245472e+00 #> 8      qsec -1.415215e+00 9.161317e-01    t -1.54477235 0.9232827 -1.415215e+00 #> 9        vs -2.355400e+00 1.453909e+00    t -1.62004564 0.9318547 -2.355400e+00 #> 10       wt  1.728712e-01 1.907088e+00    t  0.09064671 0.4647817  1.728712e-01 #>            ci.lo #> 1  -5.856338e-01 #> 2  -8.605517e-05 #> 3  -1.203758e+01 #> 4  -6.677870e-03 #> 5  -7.692768e+00 #> 6  -9.960130e+00 #> 7  -3.743222e+00 #> 8  -3.075668e+00 #> 9  -4.990554e+00 #> 10 -3.283652e+00  # Classification with logistic regression, log-loss and cross validation cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),      resampling = rsmp(\"cv\", folds = 5),      measure = \"classif.logloss\", test = \"t\") #>       Variable           CPI           SE test  statistic      p.value #> 1 Petal.Length -2.088854e-06 2.396937e-06    t -0.8714677 0.8075495728 #> 2  Petal.Width  8.340591e-04 2.311171e-03    t  0.3608815 0.3593495756 #> 3 Sepal.Length  0.000000e+00 0.000000e+00    t        NaN          NaN #> 4  Sepal.Width  2.649363e-02 7.817222e-03    t  3.3891360 0.0004486843 #>        estimate         ci.lo #> 1 -2.088854e-06 -6.056132e-06 #> 2  8.340591e-04 -2.991264e-03 #> 3  0.000000e+00           NaN #> 4  2.649363e-02  1.355500e-02   # Use your own data (and out-of-bag loss with random forest) mytask <- as_task_classif(iris, target = \"Species\") mylearner <- lrn(\"classif.ranger\", predict_type = \"prob\", keep.inbag = TRUE) cpi(task = mytask, learner = mylearner,      resampling = \"oob\", measure = \"classif.logloss\") #>       Variable           CPI           SE test  statistic   p.value #> 1 Petal.Length -2.177212e-03 0.0020600892    t -1.0568533 0.8538555 #> 2  Petal.Width -5.096020e-03 0.0122905073    t -0.4146305 0.6604956 #> 3 Sepal.Length  6.097072e-05 0.0003490978    t  0.1746523 0.4307950 #> 4  Sepal.Width -1.377991e-03 0.0028165985    t -0.4892395 0.6873042 #>        estimate         ci.lo #> 1 -2.177212e-03 -0.0055869576 #> 2 -5.096020e-03 -0.0254385857 #> 3  6.097072e-05 -0.0005168366 #> 4 -1.377991e-03 -0.0060398689      # Group CPI cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),      resampling = rsmp(\"cv\", folds = 5),      groups = list(Sepal = 1:2, Petal = 3:4)) #>   Group          CPI          SE test statistic      p.value     estimate #> 1 Sepal 0.0008325623 0.002184879    t 0.3810564 0.3518520910 0.0008325623 #> 2 Petal 0.0258880900 0.007723130    t 3.3520206 0.0005084659 0.0258880900 #>          ci.lo #> 1 -0.002783729 #> 2  0.013105194      if (FALSE) { # Bayesian testing res <- cpi(task = tsk(\"iris\"),             learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),             resampling = rsmp(\"holdout\"),             measure = \"classif.logloss\", test = \"bayes\") plot(res$Petal.Length)  # Parallel execution doParallel::registerDoParallel(4) cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),      resampling = rsmp(\"cv\", folds = 5))      # Use sequential knockoffs for categorical features # package available here: https://github.com/kormama1/seqknockoff mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\"),      resampling = rsmp(\"holdout\"),      knockoff_fun = seqknockoff::knockoffs_seq) }"},{"path":"https://dswatson.github.io/cpi/news/index.html","id":"cpi-010","dir":"Changelog","previous_headings":"","what":"cpi 0.1.0","title":"cpi 0.1.0","text":"Migrate mlr3 framework Allow data.frame inputs Add interface knockoff samplers Add group CPI","code":""}]
