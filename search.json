[{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"get-started","dir":"Articles","previous_headings":"","what":"Get started","title":"Introduction to the cpi package","text":"Conditional Predictive Impact (CPI) general test conditional independence supervised learning algorithms. implements conditional variable importance measure can applied supervised learning algorithm loss function. first example, calculate CPI random forest wine data 5-fold cross validation: result CPI value feature, .e. much loss function change feature replaced knockoff version, corresponding standard errors, test statistics, p-values confidence interval.","code":"library(mlr3) library(mlr3learners) library(cpi)  cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\", num.trees = 10),     resampling = rsmp(\"cv\", folds = 5)) #>           Variable      CPI      SE test statistic estimate p.value    ci.lo #> 1       alcalinity  0.00106 0.00346    t      0.31  0.00106  0.3798 -0.00466 #> 2          alcohol  0.02759 0.01088    t      2.54  0.02759  0.0060  0.00961 #> 3              ash  0.00019 0.00019    t      1.00  0.00019  0.1593 -0.00012 #> 4            color  0.21308 0.18515    t      1.15  0.21308  0.1257 -0.09306 #> 5         dilution  0.00046 0.00771    t      0.06  0.00046  0.4761 -0.01229 #> 6       flavanoids  0.00000 0.00000    t      0.00  0.00000  1.0000  0.00000 #> 7              hue  0.00151 0.00705    t      0.21  0.00151  0.4155 -0.01015 #> 8        magnesium  0.00826 0.00494    t      1.67  0.00826  0.0480  0.00010 #> 9            malic  0.00047 0.00412    t      0.11  0.00047  0.4551 -0.00635 #> 10   nonflavanoids  0.00073 0.00205    t      0.36  0.00073  0.3612 -0.00266 #> 11         phenols -0.00351 0.00346    t     -1.01 -0.00351  0.8441 -0.00922 #> 12 proanthocyanins  0.00162 0.00389    t      0.42  0.00162  0.3389 -0.00481 #> 13         proline  0.08475 0.03003    t      2.82  0.08475  0.0027  0.03509"},{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"interface-with-mlr3","dir":"Articles","previous_headings":"","what":"Interface with mlr3","title":"Introduction to the cpi package","text":"task, learner resampling strategy specified mlr3 package, provides unified interface machine learning tasks makes quite easy change components. example, can change regularized logistic regression simple holdout resampling strategy: refer mlr3 book full introduction reference. loss function used cpi() function specified measure. default, mean squared error (MSE) used regression log-loss classification. mlr3, corresponds measures \"regr.mse\" \"classif.logloss\". re-run example simple classification error (ce): see 0 CPI values classification error less sensitive small changes hence results lower power.","code":"cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.01),     resampling = rsmp(\"holdout\")) #>           Variable      CPI      SE test statistic estimate p.value    ci.lo #> 1       alcalinity  8.6e-03 1.4e-02    t      0.62  8.6e-03   0.269 -1.5e-02 #> 2          alcohol  2.0e-02 1.4e-02    t      1.46  2.0e-02   0.074 -2.8e-03 #> 3              ash  3.1e-04 2.8e-04    t      1.11  3.1e-04   0.136 -1.6e-04 #> 4            color  3.2e-02 2.1e-02    t      1.57  3.2e-02   0.061 -2.2e-03 #> 5         dilution  8.2e-03 7.2e-03    t      1.15  8.2e-03   0.128 -3.8e-03 #> 6       flavanoids  4.0e-06 4.0e-06    t      1.01  4.0e-06   0.157 -2.6e-06 #> 7              hue  6.4e-03 8.1e-03    t      0.79  6.4e-03   0.217 -7.1e-03 #> 8        magnesium  0.0e+00 0.0e+00    t      0.00  0.0e+00   1.000  0.0e+00 #> 9            malic -8.0e-03 9.3e-03    t     -0.85 -8.0e-03   0.802 -2.4e-02 #> 10   nonflavanoids -9.6e-04 2.9e-03    t     -0.33 -9.6e-04   0.627 -5.9e-03 #> 11         phenols  0.0e+00 0.0e+00    t      0.00  0.0e+00   1.000  0.0e+00 #> 12 proanthocyanins  0.0e+00 0.0e+00    t      0.00  0.0e+00   1.000  0.0e+00 #> 13         proline  3.5e-03 1.8e-02    t      0.19  3.5e-03   0.424 -2.7e-02 cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.glmnet\", lambda = 0.01),     resampling = rsmp(\"holdout\"),      measure = msr(\"classif.ce\")) #>           Variable    CPI    SE test statistic estimate p.value  ci.lo #> 1       alcalinity  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 2          alcohol  0.017 0.030    t      0.57    0.017    0.28 -0.032 #> 3              ash  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 4            color  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 5         dilution -0.017 0.017    t     -1.00   -0.017    0.84 -0.045 #> 6       flavanoids  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 7              hue  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 8        magnesium -0.017 0.017    t     -1.00   -0.017    0.84 -0.045 #> 9            malic -0.017 0.017    t     -1.00   -0.017    0.84 -0.045 #> 10   nonflavanoids -0.017 0.017    t     -1.00   -0.017    0.84 -0.045 #> 11         phenols  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 12 proanthocyanins  0.000 0.000    t      0.00    0.000    1.00  0.000 #> 13         proline  0.017 0.045    t      0.38    0.017    0.35 -0.059"},{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"statistical-testing","dir":"Articles","previous_headings":"","what":"Statistical testing","title":"Introduction to the cpi package","text":"CPI offers several statistical tests calculated: t-test (\"t\", default), Wilcoxon signed-rank test (\"wilcox\"), binomial test (\"binom\"), Fisher permutation test (\"fisher\") Bayesian testing (\"bayes\") package BEST. example, re-run first example Fisher’s permutation test:","code":"cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\", num.trees = 10),     resampling = rsmp(\"cv\", folds = 5),      test = \"fisher\") #>           Variable      CPI      SE   test p.value    ci.lo #> 1       alcalinity  0.00864 0.00441 fisher  0.0255  0.00131 #> 2          alcohol  0.02225 0.01082 fisher  0.0180  0.00459 #> 3              ash  0.00000 0.00000 fisher  1.0000  0.00000 #> 4            color  0.03918 0.01555 fisher  0.0040  0.01390 #> 5         dilution  0.00864 0.00780 fisher  0.1395 -0.00435 #> 6       flavanoids -0.00039 0.00039 fisher  1.0000 -0.00078 #> 7              hue  0.00720 0.00738 fisher  0.1700 -0.00473 #> 8        magnesium  0.00344 0.00366 fisher  0.1635 -0.00249 #> 9            malic  0.01378 0.00394 fisher  0.0005  0.00700 #> 10   nonflavanoids -0.00118 0.00289 fisher  0.6405 -0.00614 #> 11         phenols -0.00192 0.00431 fisher  0.6695 -0.00938 #> 12 proanthocyanins  0.00926 0.00476 fisher  0.0220  0.00128 #> 13         proline  0.05066 0.01421 fisher  0.0005  0.02737"},{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"knockoff-procedures","dir":"Articles","previous_headings":"","what":"Knockoff procedures","title":"Introduction to the cpi package","text":"CPI relies valid knockoff sampler data analyzed. default, second-order Gaussian knockoffs package knockoff used. However, knockoff sampler can used changing knockoff_fun x_tilde argument cpi() function. , knockoff_fun expects function taking data.frame original data input returning data.frame knockoffs. example, use sequential knockoffs seqknockoff package1: x_tilde argument directly takes knockoff data:","code":"mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\", num.trees = 10),      resampling = rsmp(\"cv\", folds = 5),      knockoff_fun = seqknockoff::knockoffs_seq) library(seqknockoff) x_tilde <- knockoffs_seq(iris[, -3]) mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\", num.trees = 10),      resampling = rsmp(\"cv\", folds = 5),      x_tilde = x_tilde)"},{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"group-cpi","dir":"Articles","previous_headings":"","what":"Group CPI","title":"Introduction to the cpi package","text":"Instead calculating CPI feature separately, can also calculate groups features replacing data whole groups respective knockoff data. cpi() can done groups argument:","code":"cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.01),     resampling = rsmp(\"holdout\"),      groups = list(Sepal = 1:2, Petal = 3:4)) #>   Group    CPI    SE test statistic estimate p.value   ci.lo #> 1 Sepal 0.0033 0.009    t      0.37   0.0033   0.358 -0.0118 #> 2 Petal 0.0192 0.011    t      1.82   0.0192   0.037  0.0015"},{"path":"https://bips-hb.github.io/cpi/articles/intro.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"Parallelization","title":"Introduction to the cpi package","text":"parallel execution, need register parallel backend. Parallelization performed features, .e. CPI feature calculated parallel. example:","code":"doParallel::registerDoParallel(4) cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\", num.trees = 10),     resampling = rsmp(\"cv\", folds = 5))"},{"path":"https://bips-hb.github.io/cpi/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author.","code":""},{"path":"https://bips-hb.github.io/cpi/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Watson, D.S. & Wright, M.N. (2021). Testing conditional independence supervised learning algorithms. Machine Learning 110(8):2107-2129. doi:10.1007/s10994-021-06030-6","code":"@Article{,   title = {Testing conditional independence in supervised learning algorithms},   author = {David S. Watson and Marvin N. Wright},   journal = {Machine Learning},   year = {2011},   volume = {110},   number = {8},   pages = {2107--2129},   url = {https://doi.org/10.1007/s10994-021-06030-6}, }"},{"path":"https://bips-hb.github.io/cpi/index.html","id":"conditional-predictive-impact","dir":"","previous_headings":"","what":"Conditional Predictive Impact","title":"Conditional Predictive Impact","text":"David S. Watson, Marvin N. Wright","code":""},{"path":"https://bips-hb.github.io/cpi/index.html","id":"introduction","dir":"","previous_headings":"Conditional Predictive Impact","what":"Introduction","title":"Conditional Predictive Impact","text":"conditional predictive impact (CPI) measure conditional independence. can calculated using supervised learning algorithm, loss function, knockoff sampler. provide statistical inference procedures CPI without parametric assumptions sparsity constraints. method works continuous categorical data.","code":""},{"path":"https://bips-hb.github.io/cpi/index.html","id":"installation","dir":"","previous_headings":"Conditional Predictive Impact","what":"Installation","title":"Conditional Predictive Impact","text":"package CRAN yet. install development version GitHub using devtools, run","code":"devtools::install_github(\"bips-hb/cpi\")"},{"path":"https://bips-hb.github.io/cpi/index.html","id":"examples","dir":"","previous_headings":"Conditional Predictive Impact","what":"Examples","title":"Conditional Predictive Impact","text":"Calculate CPI random forest iris data 5-fold cross validation:","code":"library(mlr3) library(mlr3learners) library(cpi)  cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\"),     resampling = rsmp(\"cv\", folds = 5),      measure = \"classif.logloss\", test = \"t\")"},{"path":"https://bips-hb.github.io/cpi/index.html","id":"references","dir":"","previous_headings":"Conditional Predictive Impact","what":"References","title":"Conditional Predictive Impact","text":"Watson D. S. & Wright, M. N. (2021). Testing conditional independence supervised learning algorithms. Machine Learning. DOI: 10.1007/s10994-021-06030-6.","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Predictive Impact (CPI). — cpi","title":"Conditional Predictive Impact (CPI). — cpi","text":"general test conditional  independence supervised learning algorithms. Implements conditional  variable importance measure can applied supervised learning  algorithm loss function. Provides statistical inference procedures  without parametric assumptions applies equally well continuous  categorical predictors outcomes.","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Predictive Impact (CPI). — cpi","text":"","code":"cpi(   task,   learner,   resampling = NULL,   test_data = NULL,   measure = NULL,   test = \"t\",   log = FALSE,   B = 1999,   alpha = 0.05,   x_tilde = NULL,   knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)),   groups = NULL,   verbose = FALSE )"},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Predictive Impact (CPI). — cpi","text":"task prediction mlr3 task, see examples. learner mlr3 learner used CPI. pass string,  learner created via mlr3::lrn. resampling Resampling strategy, mlr3 resampling object  (e.g. rsmp(\"holdout\")), \"oob\" (--bag) \"none\"  (-sample loss). test_data External validation data, use instead resampling. measure Performance measure (loss). Per default, use MSE  (\"regr.mse\") regression logloss (\"classif.logloss\")  classification. test Statistical test perform, one \"t\" (t-test, default),  \"wilcox\" (Wilcoxon signed-rank test), \"binom\" (binomial  test), \"fisher\" (Fisher permutation test) \"bayes\"  (Bayesian testing, computationally intensive!). See Details. log Set TRUE multiplicative CPI (\\(\\lambda\\)),  FALSE (default) additive CPI (\\(\\Delta\\)). B Number permutations Fisher permutation test. alpha Significance level confidence intervals. x_tilde Knockoff matrix data.frame. given (default),  created function given knockoff_fun. knockoff_fun Function generate knockoffs. Default:  knockoff::create.second_order matrix argument. groups (Named) list groups. Set NULL (default) groups, .e. compute CPI feature. See examples. verbose Verbose output resampling procedure.","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Predictive Impact (CPI). — cpi","text":"test = \"bayes\" list BEST objects.  case, data.frame row feature columns: Variable/Group Variable/group name CPI CPI value SE Standard error test Testing method statistic Test statistic (t-test, Wilcoxon binomial test) estimate Estimated mean (t-test), median (Wilcoxon test),     proportion \\(\\Delta\\)-values greater 0 (binomial test). p.value p-value ci.lo Lower limit (1 - alpha) * 100% confidence interval Note NA values error result CPI value 0, .e.  difference model performance replacing feature knockoff.","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Predictive Impact (CPI). — cpi","text":"function computes conditional predictive impact (CPI) one several features given supervised learning task. represents  mean error inflation replacing true variable knockoff. Large CPI values evidence feature(s) question high  conditional variable importance -- .e., fitted model relies  feature(s) predict outcome, even accounting signal remaining covariates. build mlr3 framework, provides unified interface  training models, specifying loss functions, estimating generalization  error. See package documentation info. Methods implemented frequentist Bayesian inference. default test = \"t\", fast powerful sample sizes. Wilcoxon signed-rank test (test = \"wilcox\") may appropriate  CPI distribution skewed, binomial test (test = \"binom\")  requires basically assumptions may less power. small sample  sizes, recommend permutation tests (test = \"fisher\") Bayesian  methods (test = \"bayes\"). latter case, default priors  assumed. See BEST package info. parallel execution, register backend, e.g. doParallel::registerDoParallel().","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional Predictive Impact (CPI). — cpi","text":"Watson, D. & Wright, M. (2020). Testing conditional independence  supervised learning algorithms. Machine Learning, 110(8):  2107-2129. doi: 10.1007/s10994-021-06030-6 Candès, E., Fan, Y., Janson, L, & Lv, J. (2018). Panning gold: 'model-X' knockoffs high dimensional controlled variable selection. J. R.  Statistc. Soc. B, 80(3): 551-577. doi: 10.1111/rssb.12265","code":""},{"path":"https://bips-hb.github.io/cpi/reference/cpi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Predictive Impact (CPI). — cpi","text":"","code":"library(mlr3) library(mlr3learners)  # Regression with linear model and holdout validation cpi(task = tsk(\"mtcars\"), learner = lrn(\"regr.lm\"),      resampling = rsmp(\"holdout\")) #>    Variable           CPI           SE test  statistic      estimate    p.value #> 1        am  -2.642718339 4.787049e+00    t -0.5520558  -2.642718339 0.70348368 #> 2      carb   0.000661095 2.510050e-03    t  0.2633792   0.000661095 0.39880285 #> 3       cyl   0.523106059 3.358877e-01    t  1.5573837   0.523106059 0.07521802 #> 4      disp   0.001302331 7.391687e-04    t  1.7618864   0.001302331 0.05428567 #> 5      drat -11.906944480 1.158734e+01    t -1.0275821 -11.906944480 0.83581983 #> 6      gear   0.162527235 1.263088e+00    t  0.1286745   0.162527235 0.45008315 #> 7        hp  -0.581942471 2.615023e-01    t -2.2253814  -0.581942471 0.97488286 #> 8      qsec -23.635603316 1.878338e+01    t -1.2583251 -23.635603316 0.88157022 #> 9        vs -12.093236164 1.141902e+01    t -1.0590432 -12.093236164 0.84275486 #> 10       wt   7.755317377 3.118887e+00    t  2.4865659   7.755317377 0.01608946 #>            ci.lo #> 1  -1.131906e+01 #> 2  -3.888272e-03 #> 3  -8.567737e-02 #> 4  -3.738328e-05 #> 5  -3.290855e+01 #> 6  -2.126770e+00 #> 7  -1.055905e+00 #> 8  -5.767976e+01 #> 9  -3.278977e+01 #> 10  2.102457e+00  # \\donttest{ # Classification with logistic regression, log-loss and t-test cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),      resampling = rsmp(\"holdout\"),      measure = \"classif.logloss\", test = \"t\") #>           Variable           CPI           SE test   statistic      estimate #> 1       alcalinity  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 2          alcohol  1.318305e-02 2.780654e-02    t  0.47409880  1.318305e-02 #> 3              ash  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 4            color  9.790466e-03 3.502169e-03    t  2.79554333  9.790466e-03 #> 5         dilution  9.441094e-03 1.365005e-02    t  0.69165278  9.441094e-03 #> 6       flavanoids -1.866027e-06 6.741879e-06    t -0.27678148 -1.866027e-06 #> 7              hue -9.329660e-04 1.214049e-02    t -0.07684745 -9.329660e-04 #> 8        magnesium  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 9            malic  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 10   nonflavanoids  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 11         phenols  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 12 proanthocyanins  0.000000e+00 0.000000e+00    t  0.00000000  0.000000e+00 #> 13         proline  4.801290e-02 2.204620e-02    t  2.17783122  4.801290e-02 #>        p.value         ci.lo #> 1  1.000000000  0.000000e+00 #> 2  0.318604722 -3.329706e-02 #> 3  1.000000000  0.000000e+00 #> 4  0.003507218  3.936405e-03 #> 5  0.245957440 -1.337568e-02 #> 6  0.608534042 -1.313543e-05 #> 7  0.530495309 -2.122644e-02 #> 8  1.000000000  0.000000e+00 #> 9  1.000000000  0.000000e+00 #> 10 1.000000000  0.000000e+00 #> 11 1.000000000  0.000000e+00 #> 12 1.000000000  0.000000e+00 #> 13 0.016749322  1.116152e-02   # Use your own data (and out-of-bag loss with random forest) mytask <- as_task_classif(iris, target = \"Species\") mylearner <- lrn(\"classif.ranger\", predict_type = \"prob\", keep.inbag = TRUE) cpi(task = mytask, learner = mylearner,      resampling = \"oob\", measure = \"classif.logloss\") #>       Variable           CPI           SE test   statistic      estimate #> 1 Petal.Length  0.0024539740 0.0034245712    t  0.71657848  0.0024539740 #> 2  Petal.Width  0.0113491395 0.0234543474    t  0.48388213  0.0113491395 #> 3 Sepal.Length -0.0005998955 0.0003309134    t -1.81284749 -0.0005998955 #> 4  Sepal.Width -0.0001980922 0.0036512881    t -0.05425268 -0.0001980922 #>     p.value        ci.lo #> 1 0.2373780 -0.003214186 #> 2 0.3145900 -0.027471195 #> 3 0.9640659 -0.001147605 #> 4 0.5215967 -0.006241502      # Group CPI cpi(task = tsk(\"iris\"),      learner = lrn(\"classif.ranger\", predict_type = \"prob\", num.trees = 10),      resampling = rsmp(\"cv\", folds = 3),      groups = list(Sepal = 1:2, Petal = 3:4)) #>   Group         CPI          SE test statistic    estimate    p.value #> 1 Sepal 0.449810950 0.304452449    t  1.477442 0.449810950 0.07083385 #> 2 Petal 0.002810199 0.002026415    t  1.386784 0.002810199 0.08378947 #>           ci.lo #> 1 -0.0541018558 #> 2 -0.0005438102 # }      if (FALSE) {       # Bayesian testing res <- cpi(task = tsk(\"iris\"),             learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),             resampling = rsmp(\"holdout\"),             measure = \"classif.logloss\", test = \"bayes\") plot(res$Petal.Length)  # Parallel execution doParallel::registerDoParallel() cpi(task = tsk(\"wine\"),      learner = lrn(\"classif.glmnet\", predict_type = \"prob\", lambda = 0.1),      resampling = rsmp(\"cv\", folds = 5))      # Use sequential knockoffs for categorical features # package available here: https://github.com/kormama1/seqknockoff mytask <- as_task_regr(iris, target = \"Petal.Length\") cpi(task = mytask, learner = lrn(\"regr.ranger\"),      resampling = rsmp(\"holdout\"),      knockoff_fun = seqknockoff::knockoffs_seq) }"},{"path":"https://bips-hb.github.io/cpi/news/index.html","id":"cpi-014","dir":"Changelog","previous_headings":"","what":"cpi 0.1.4","title":"cpi 0.1.4","text":"Reset options() vignette","code":""},{"path":"https://bips-hb.github.io/cpi/news/index.html","id":"cpi-013","dir":"Changelog","previous_headings":"","what":"cpi 0.1.3","title":"cpi 0.1.3","text":"Save/reset previous mlr3 logging level","code":""},{"path":"https://bips-hb.github.io/cpi/news/index.html","id":"cpi-012","dir":"Changelog","previous_headings":"","what":"cpi 0.1.2","title":"cpi 0.1.2","text":"Speedup examples CRAN","code":""},{"path":"https://bips-hb.github.io/cpi/news/index.html","id":"cpi-011","dir":"Changelog","previous_headings":"","what":"cpi 0.1.1","title":"cpi 0.1.1","text":"Set test statistics 0 CPI=0","code":""},{"path":"https://bips-hb.github.io/cpi/news/index.html","id":"cpi-010","dir":"Changelog","previous_headings":"","what":"cpi 0.1.0","title":"cpi 0.1.0","text":"Migrate mlr3 framework Allow data.frame inputs Add interface knockoff samplers Add group CPI","code":""}]
